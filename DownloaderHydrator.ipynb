{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet ID Downloader\n",
    "# This program will download the tweet IDs from the CoVaxxy dataset\n",
    "\n",
    "import requests\n",
    "import os \n",
    "from os import path\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "cwd = str(os.getcwd())\n",
    "covaxxycwd = cwd + \"\\\\CoVaxxyDataSet\"\n",
    "\n",
    "# This section will check to see if the CoVaxxyDataSet folder exists in the current working directory\n",
    "if \"CoVaxxyDataSet\" in cwd:\n",
    "    print ('The working directory is: \\n' + cwd)\n",
    "else:\n",
    "    if not os.path.exists(covaxxycwd):\n",
    "        print(\"\\nThe file does not exist\")\n",
    "        os.makedirs(covaxxycwd)\n",
    "        print(\"\\nCoVaxxyDataSet was created in \\n\" +cwd)\n",
    "    elif os.path.exists(covaxxycwd):\n",
    "        print(\"\\nThe file CoVaxxyDataSet exists in the current working directory: \\n\" + str(os.getcwd()))\n",
    "    os.chdir(covaxxycwd)\n",
    "    print(\"\\nThe working directory has been set to: \\n\" +covaxxycwd)\n",
    "\n",
    "print ('\\nThe working directory is correct')\n",
    "for i in range (0,4):\n",
    "    print('.')\n",
    "    time.sleep(0.3)\n",
    "print('Beginning Downloads')\n",
    "IntialDate = dt.datetime(2021,1,4)\n",
    "FinalDate = dt.datetime.today()\n",
    "url1 = 'https://zenodo.org/record/5035808/files/tweet_ids--'\n",
    "url2 = '.txt?download=1'\n",
    "#establishing the download URL\n",
    "\n",
    "# This section will calculate the number of days between the initial date and the final date for Twitter data collection\n",
    "TimeDelta = FinalDate - IntialDate\n",
    "TimeDelta = int(str(TimeDelta)[0:3])\n",
    "\n",
    "# This section will download the data from the URL and save it to the current working directory\n",
    "Date = IntialDate\n",
    "for i in range(0,TimeDelta): \n",
    "    Date = Date + dt.timedelta(days = 1)\n",
    "    Datestr = str(Date)[0:10]\n",
    "    Url = url1 + Datestr + url2\n",
    "    FileName =url1[40:]+Datestr+url2[0:4]\n",
    "    time.sleep(0.02)\n",
    "    if path.exists(FileName) is False:\n",
    "        r = requests.get(Url,allow_redirects=True)\n",
    "        open(FileName,'wb').write(r.content)\n",
    "        print(FileName + \" Has been successfully downloaded.\")\n",
    "    else:\n",
    "        print(FileName + \" exists and was not over written.\")\n",
    "    \n",
    "print(\"\\nDataset is complete as of \" +str(FinalDate)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section will combine all of the files into one file\n",
    "#@title Set up Directory { run: \"auto\"}\n",
    "final_tweet_ids_filename = \"tweet_ids--2021-01-11.txt\" #@param {type: \"string\"}\n",
    "output_filename = \"2021-01-11.csv\" #@param {type: \"string\"}\n",
    "\n",
    "# This section extracts the tweet IDs from the downloaded files and saves them to a single file for use in the Hydrator\n",
    "output_json_filename = output_filename[:output_filename.index(\".\")] + \".txt\"\n",
    "ids = []\n",
    "with open(final_tweet_ids_filename, \"r\") as ids_file:\n",
    "    ids = ids_file.read().split()\n",
    "hydrated_tweets = []\n",
    "ids_to_hydrate = set(ids)\n",
    "if os.path.isfile(output_json_filename):\n",
    "    with jsonlines.open(output_json_filename, \"r\") as reader:\n",
    "        for i in reader.iter(type=dict, skip_invalid=True):\n",
    "            hydrated_tweets.append(i)\n",
    "            ids_to_hydrate.remove(i[\"id_str\"])\n",
    "print(\"Total IDs: \" + str(len(ids)) + \", IDs to hydrate: \" + str(len(ids_to_hydrate)))\n",
    "print(\"Hydrated: \" + str(len(hydrated_tweets)))\n",
    "\n",
    "count = len(hydrated_tweets)\n",
    "start_index = count;\n",
    "num_save  = 1000\n",
    "\n",
    "# This section will hydrate the tweet IDs and save them to a file\n",
    "for tweet in twarc.hydrate(ids_to_hydrate):\n",
    "    hydrated_tweets.append(tweet)\n",
    "    count += 1\n",
    "    if (count % num_save) == 0:\n",
    "        with jsonlines.open(output_json_filename, \"a\") as writer:\n",
    "            print(\"Started IO\")\n",
    "            for hydrated_tweet in hydrated_tweets[start_index:]:\n",
    "                writer.write(hydrated_tweet)\n",
    "            print(\"Finished IO\")\n",
    "        print(\"Saved \" + str(count) + \" hydrated tweets.\")\n",
    "        start_index = count\n",
    "if count != start_index:\n",
    "    print(\"Here with start_index\", start_index)\n",
    "    with jsonlines.open(output_json_filename, \"a\") as writer:\n",
    "        for hydrated_tweet in hydrated_tweets[start_index:]:\n",
    "            writer.write(hydrated_tweet)\n",
    "\n",
    "# This section will convert the json file to a csv file\n",
    "import csv, jsonlines\n",
    "output_json_filename = output_filename[:output_filename.index(\".\")] + \".txt\"\n",
    "keyset = [\"created_at\", \"id\", \"id_str\", \"full_text\", \"source\", \"truncated\", \"in_reply_to_status_id\",\n",
    "          \"in_reply_to_status_id_str\", \"in_reply_to_user_id\", \"in_reply_to_user_id_str\", \n",
    "          \"in_reply_to_screen_name\", \"user\", \"coordinates\", \"place\", \"quoted_status_id\",\n",
    "          \"quoted_status_id_str\", \"is_quote_status\", \"quoted_status\", \"retweeted_status\", \n",
    "          \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\", \"entities\", \n",
    "          \"extended_entities\", \"favorited\", \"retweeted\", \"possibly_sensitive\", \"filter_level\", \n",
    "          \"lang\", \"matching_rules\", \"current_user_retweet\", \"scopes\", \"withheld_copyright\", \n",
    "          \"withheld_in_countries\", \"withheld_scope\", \"geo\", \"contributors\", \"display_text_range\",\n",
    "          \"quoted_status_permalink\"]\n",
    "hydrated_tweets = []\n",
    "with jsonlines.open(output_json_filename, \"r\") as reader:\n",
    "    for i in reader.iter(type=dict, skip_invalid=True):\n",
    "        hydrated_tweets.append(i)\n",
    "        \n",
    "# This section will create a folder in the current working directory to store the csv file\n",
    "cwd = str(os.getcwd())\n",
    "csvcwd = cwd + \"//CsvData\"\n",
    "if \"CsvData\" in cwd:\n",
    "    print ('The working directory is: \\n' + cwd)\n",
    "else:\n",
    "    if not os.path.exists(csvcwd):\n",
    "        print(\"\\nThe file does not exist\")\n",
    "        os.makedirs(csvcwd)\n",
    "        print(\"\\CsvData was created in \\n\" +cwd)\n",
    "    elif os.path.exists(csvcwd):\n",
    "        print(\"\\nThe file CsvData exists in the current working directory: \\n\" + str(os.getcwd()))\n",
    "    os.chdir(csvcwd)\n",
    "    print(\"\\nThe working directory has been set to: \\n\" +csvcwd)\n",
    "        \n",
    "with  open(output_filename, \"w+\") as output_file:\n",
    "    d = csv.DictWriter(output_file, keyset)\n",
    "    d.writeheader()\n",
    "    d.writerows(hydrated_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is for data agrigation\n",
    "\n",
    "# This section will create a folder in the current working directory to store the csv file\n",
    "import pandas as pd , glob, os, logging\n",
    "from gapminder import gapminder\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "LogFileName = \"Version \"+str(version)+'.log'\n",
    "logging.basicConfig(filename=LogFileName,level=logging.DEBUG,\n",
    "                    format='%(asctime)s: %(levelname)s:  %(message)s',\n",
    "                    datefmt='%m-%d-%Y %I:%M:%S %p')\n",
    "\n",
    "print(\"Version: \" + str(Ver))\n",
    "logging.info(\"The working directory currently is: {}\".format(str(os.getcwd())))\n",
    "Path = \"C:\\\\Users\\\\2256209\\\\OneDrive\\\\Desktop\\\\CSV\\\\\"\n",
    "os.chdir(Path)\n",
    "logging.info('The working directory is now set to : {}'.format(str(os.getcwd())))\n",
    "IffyPath = \"C:\\\\Users\\\\2256209\\\\OneDrive\\\\Desktop\\\\IffyIndex.csv\"\n",
    "IffyDataframe = pd.read_csv(IffyPath,header=0, index_col = \"Index\")\n",
    "joined_files = os.path.join(Path, \"*.csv\")\n",
    "joined_list = glob.glob(joined_files)\n",
    "\n",
    "x = 0\n",
    "\n",
    "TotalTweetsAnalyzed = 0\n",
    "FilteredTweets = 0\n",
    "for i in range(0,len(joined_list)):\n",
    "    logging.info(\"Selected File: {}   File {} of {}\".format(joined_list[x], x, len(joined_list)))\n",
    "    CurrentDataframe = pd.read_csv(joined_list[x],header=0)\n",
    "    FilteredCurrentDataframe1 = CurrentDataframe[CurrentDataframe.urls.notnull() & CurrentDataframe.user_location.notnull()]\n",
    "    del CurrentDataframe\n",
    "    FilteredCurrentDataframe = FilteredCurrentDataframe1.drop(columns = ['coordinates',\n",
    "                                                                        'media',\n",
    "                                                                        'favorite_count',\n",
    "                                                                        'in_reply_to_screen_name',\n",
    "                                                                        'in_reply_to_status_id',\n",
    "                                                                        'in_reply_to_user_id',\n",
    "                                                                        'user_followers_count',\n",
    "                                                                        'user_friends_count',\n",
    "                                                                        'user_listed_count',\n",
    "                                                                        'user_name',\n",
    "                                                                        'user_screen_name.1',\n",
    "                                                                        'user_statuses_count',\n",
    "                                                                        'user_time_zone',\n",
    "                                                                        'user_urls',\n",
    "                                                                        'lang',\n",
    "                                                                        'place',\n",
    "                                                                        'user_favourites_count',\n",
    "                                                                        'user_favourites_count',\n",
    "                                                                        'user_description',\n",
    "                                                                        'user_default_profile_image',\n",
    "                                                                        'user_screen_name',\n",
    "                                                                        'user_created_at',\n",
    "                                                                        'tweet_url',\n",
    "                                                                        'text',\n",
    "                                                                        'source',\n",
    "                                                                        'retweet_screen_name',\n",
    "                                                                        'retweet_id',\n",
    "                                                                        'retweet_count',\n",
    "                                                                        'possibly_sensitive',\n",
    "                                                                        'user_verified'])\n",
    "    del FilteredCurrentDataframe1\n",
    "    logging.info(\"The current file: {} has been filtered\".format(joined_list[x]))\n",
    "    \n",
    "    \n",
    "    y = 0\n",
    "    for j in range(0,FilteredCurrentDataframe.shape[0]):\n",
    "        FilteredCurrentDataframe.created_at.iloc[y] = joined_list[x][57:67]\n",
    "        y = y +1\n",
    "    logging.info(\"The current file: {} date has been formated\".format(joined_list[x]))\n",
    "\n",
    "    z = 0\n",
    "    a = 0\n",
    "    BaseUrlList = []\n",
    "    for k in range(0,FilteredCurrentDataframe.shape[0]):\n",
    "        UrlString = FilteredCurrentDataframe.urls.iloc[z]\n",
    "        Fslashtot = 0\n",
    "        Fslash = 0\n",
    "        Dtot = 0\n",
    "        DubUCtr = 0\n",
    "        for l in range(0,len(UrlString)):\n",
    "            if UrlString[l] == '/':\n",
    "                Fslashtot = Fslashtot +1\n",
    "            if UrlString[l] == 'w' or UrlString[l] == 'W':\n",
    "                DubUCtr = DubUCtr + 1\n",
    "            elif UrlString[l] == '.' and Dtot == 0 and DubUCtr >1:\n",
    "                FrontLocation = l\n",
    "                Dtot = Dtot + 1\n",
    "            elif UrlString[l] == '/' and Dtot == 0 and DubUCtr == 0 and Fslashtot == 2:\n",
    "                FrontLocation = l\n",
    "                Dtot = Dtot + 1\n",
    "        if Fslashtot >2:\n",
    "            for m in range(0,len(UrlString)):\n",
    "                if UrlString[m] == '/':\n",
    "                    Fslash = Fslash + 1\n",
    "                    if Fslash == 3:\n",
    "                        GoodBaseUrl = UrlString[FrontLocation+1:m]\n",
    "                        GoodBaseUrl1 = UrlString[:m]\n",
    "        if Fslashtot == 2:\n",
    "            GoodBaseUrl = UrlString\n",
    "        BaseUrlList.append(GoodBaseUrl)\n",
    "        z = z+1\n",
    "    FilteredCurrentDataframe = FilteredCurrentDataframe.assign(BaseUrl = BaseUrlList)\n",
    "    logging.info(\"The current file: {} BaseURL has been extracted\".format(joined_list[x]))\n",
    "\n",
    "    \n",
    "    # Assigning the credibility score to the URL from the IFFY index. \n",
    "    b = 0\n",
    "    CredibilityCTR = 0\n",
    "    PercentageTracker = 0\n",
    "    URLPotentialMatches = FilteredCurrentDataframe.shape[0] * IffyDataframe.shape[0]\n",
    "    \n",
    "    MBFCList = []\n",
    "    for n in range(0,FilteredCurrentDataframe.shape[0]):\n",
    "\n",
    "        c = 0\n",
    "        MBFCListAppendBit = 0\n",
    "        for p in range(0,IffyDataframe.shape[0]):\n",
    "            if IffyDataframe.Domain[c] == FilteredCurrentDataframe.BaseUrl.iloc[b]:\n",
    "                MBFCList.append(IffyDataframe.MBFCfactual[c])\n",
    "                MBFCListAppendBit = 1\n",
    "                AppendPosition = c\n",
    "            c = c + 1\n",
    "            CredibilityCTR = CredibilityCTR + 1\n",
    "        if MBFCListAppendBit == 0: \n",
    "            MBFCList.append(\"NA\")\n",
    "        b = b + 1\n",
    "        CredibilityCTR = CredibilityCTR + 1\n",
    "        CredibilityCompletion = (CredibilityCTR/URLPotentialMatches)*100\n",
    "        \n",
    "        if 20<CredibilityCompletion<30 and PercentageTracker ==0:\n",
    "            PercentageTracker = PercentageTracker + 1\n",
    "            logging.info(\"The current file Credibility Score: {} is {}% complete\".format(joined_list[x]),CredibilityCompletion)\n",
    "        \n",
    "        if 45<CredibilityCompletion<55 and PercentageTracker == 1 :\n",
    "            PercentageTracker = PercentageTracker + 1\n",
    "            logging.info(\"The current file Credibility Score: {} is {}% complete\".format(joined_list[x]),CredibilityCompletion)\n",
    "        \n",
    "        if 70<CredibilityCompletion<80 and PercentageTracker == 2:\n",
    "            PercentageTracker = PercentageTracker + 1\n",
    "            logging.info(\"The current file Credibility Score: {} is {}% complete\".format(joined_list[x]),CredibilityCompletion)\n",
    "            \n",
    "    FilteredCurrentDataframe = FilteredCurrentDataframe.assign(MBFCfactual = MBFCList)\n",
    "    logging.info(\"The current file: {} Creditability score has been added\".format(joined_list[x]))\n",
    "\n",
    "    \n",
    "     #Saving this to a csv file\n",
    "    if not os.path.exists(Path+\"Filtered\\\\\"):\n",
    "        os.makedirs(Path+\"Filtered\\\\\")\n",
    "        os.chdir(Path+\"\\\\Filtered\\\\\")\n",
    "    else:\n",
    "        os.chdir(Path+\"\\\\Filtered\\\\\")\n",
    "    CSVSafeLoc = os.getcwd() + '\\\\'+joined_list[x][57:]\n",
    "    logging.info(\"CSV File Saved: {}\".format(joined_list[x][57:]))\n",
    "    logging.info(\"File {} of {} completed\".format(x, len(joined_list))\n",
    "    FilteredCurrentDataframe.to_csv(CSVSafeLoc)\n",
    "    os.chdir(Path)\n",
    "    \n",
    "    \n",
    "    TotalTweetsAnalyzed = TotalTweetsAnalyzed + CurrentDataframe.shape[0]\n",
    "    FilteredTweets = FilteredTweets + FilteredCurrentDataframe.shape[0]\n",
    "        \n",
    "    del FilteredCurrentDataframe, BaseUrlList, MBFCList, UrlString, Fslashtot, Fslash, j, k, l, m, z"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
